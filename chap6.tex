\chapter{Accelerator Generation and Integration Using Program Binaries}
\label{instrumentChap}
As usability remains to be one of the most significant obstacles for adopting FPGA computing, in this chapter, we explore the possibility of a user
transparent flow for mapping applications to systems with reconfigurable components.
Contrast with previous chapters where source codes in high level
languages are used as inputs, here we try to only use the program binaries and their runtime profiles as the design entries. 
The mechanism for integrating the accelerator back into the overall program execution is also examined. While our computational pipeline synthesis flow was to create better compute engines running on FPGA itself, in this chapter, we
further discuss the interaction between the software component and the FPGA accelerators. From the users' perspective, a flow based on
program binaries requires no source rewriting
nor recompilation, the effort needed to take advantage
of the reconfigurable platform is thus minimized. 

\section{Characteristics of the Targeted Platform}
\label{chartarg}
The general approach of translating program binaries to accelerators can certainly be applied to various heterogeneous platforms. To justify some of the design decisions made in our flow, the assumed platform 
characteristics need to be outlined first. 
In section~\ref{hetero}, a whole array of machines with reconfigurable
components were examined and there is a wide spectrum of configurations
when it comes to how tightly integrated the reconfigurable fabric is to
the processor. 
In this work, we focus on the part of the space where the general purpose processor and the reconfigurable component are loosely coupled.
Instead of being a functional unit in the processor
execution pipeline, the FPGA is used as a coprocessor for which
communication and management is assumed to be expensive.
Meanwhile, the capacity of the reconfigurable fabric can potentially be greater as it is not constrained in any way by the pipeline of the associated processor. 
Most of the off-the-self systems currently available or in development~\cite{xeonwithfpga} fall into this category. As the
costs of semiconductor manufacturing become prohibitive, 
special processors with modified pipeline accommodating
reconfigurable functional units are less likely to be built
and offered commercially, as compared to systems with conventional
processor and FPGA integrated at either chip, package or board level.

Another important factor to consider is how sharable the memory
is between the CPU and the FPGA. 
One possible configuration, as represented by the zynq SoC, has
the CPU's address space shared with the programmable logic.
The physical addresses used to access the memory are identical,
whether it comes from the CPU or the FPGA. Of course in the presence
of virtual memory, address translation needs to occur within the FPGA
before memory requests are sent out. On the other hand, there are
platforms where the FPGA has its own memory space which is
explicitly populated with the working dataset before the activation of 
the accelerator~\cite{sdaccel}. It's worth noting that this difference in programming model
is orthogonal to the actual physical configuration of the platform. For instance, CPU and FPGA located in two different sockets can have shared address space while a system based on a single chip SoC may associate the FPGA with a separate DRAM interface which are not directly
accessible by CPU. 
Given the flexibility of FPGAs, there are certainly ways to create a layer of abstraction conforming to either one of these schemes, regardless
the original expected usage model. 
In this work, we assume a shared memory space between the CPU and the FPGA, 
though a mechanism for explicit data movement is also outlined in section~\ref{dtransfer}.

%However in this work, because of the nature
%of our targeted applications, we are able to devise mechanisms to tackle explicit data

%we 
%do not make assumption about the underlying platform and try to
% devise a mechanism for each scheme. 
%we treat these
%two schemes differently, as will be discussed in section~\ref{makingAcc}.
%Finally, in any CPU+FPGA coprocessor systems, the access of data memory by the programmable logic can be seen as adding to the communication overhead between the
%accelerated part and the software components, which is a key parameter
%in determining if a part of the program should be accelerated using hardware.



\section{Acceleratable Regions In Program Binaries}
The trade-off between communication efficiency and capacity
in the reconfigurable components of the systems dictates
the granularity of the accelerators to be synthesized. 
Given the loose coupling between the programmable logic and the processor,
while it is still possible to create accelerators for small windows of dynamic instruction stream, the cost of frequently controlling and communicating
with these tiny hardware engines would nullify any performance gain
achieved. 
It is ideal to have a relatively large chunk of computation
handed off to the FPGA, which then works independently with little
intervention from the CPU, before signalling the completion of the task.
The natural targets for our flow are therefore loop nests in the program
binaries. To find the loops, we use the DyninstAPI~\cite{dyninst} to parse through
the program binaries and construct the control flow graph (CFG). There may be cases where code segments are shared by multiple loops, making it harder to statically carve out the best code region for acceleration. For these
scenarios, we can leverage runtime profile of the program to find the prevalent
loops and extract single-entry-multiple-exit loops which can undergo further
optimizing transformations. 
Meanwhile, due to the presence of statically unresolvable control flow, e.g. indirect jump,
not every part of the program can be analyzed. Several techniques were
proposed to tackle this in dynamic binary translation~\cite{}, which we can potentially
adopt. However, we do not aim to have complete coverage of the code as only the
computation heavy loops are of interest to us.
Practically speaking, the more regular loops, which are the ideal candidates for FPGA acceleration, are generally easy to detect and analyze. 
% The accelerator will be invoked only if the execution 


Another characteristic of the FPGA platform is its low clock frequency
(as compared to a typical CPU), thus to have significant speed up, there
needs to be substantial amount of parallelism extracted, which requires large windows of instruction.
In addition to instruction level parallelism within basic block or a single iteration of a loop, coarse grained parallelism 
%spanning the iteration space of loop nests 
also needs to be exploited. Blocks of loop iterations are to be executed in parallel, which implies very aggressive instruction
reordering when the accelerators are created.
%
%Meanwhile, the aggressive reordering of these instructions in creating the %accelerator
%makes any speculative execution very expensive. 
Consequently, speculative execution, which some binary-based dynamic parallelization techniques were based on, may become rather expensive. As states generated by the speculatively performed operations need to be buffered, 
the amount of space required to accommodate the massively parallelel execution engine
%exploited
can be large.
The subsequent commit of these state may also induce long delay
or require complicated hardware mechanism.
In particular, for any speculatively disambiguated memory accesses, the address streams need to be dynamically cross compared to ensure the
reordering of loads and stores are indeed valid. 
Using runtime profile can boost the confidence with which the disambiguation
is performed, but the probabilistic nature of the approach does not
relieve us the need for costly dynamic checking mechanisms. To generate
lean and fast accelerators, in this work, we try to extract a set of conditions for parallelization which does not vary with the amount of work
in the loop. In other words, we want to find computation that can 
be done in run time with a fixed, statically known cost, yet still guarantees the validity of the instruction reordering we have performed for accelerator generation. 
%This will allow us to quickly gauge if an instance of a loop nest should be executed on hardware.

\subsection{Dependencies in Loops}
To understand what characteristics a loop nest should manifests for it to be a feasible target, it is useful to start from the theory of loop dependence. As
explained in section~\ref{sec:partins}, statements cannot be parallelized or
reordered when there are RAW, WAR or WAW dependencies between them. In the context of instructions in loop nests,~\cite{Kennedy:2001:OCM:502981} defined
loop-carried and loop-independent dependencies between a pair of statements $S_1$ and $S_2$ (at least one of which is writing to memory):
\begin{itemize}
    \item Loop-carried dependency: $S_1$ accesses a common memory location 
    on one iteration of a loop, and $S_2$ accesses the same location on a subsequent iteration. 
    \item loop-independent dependency: $S_1$ and $S_2$ access the same memory location in the same loop iteration, and there is a execution path from $S_1$ to $S_2$. 
\end{itemize}
The concepts of \textit{iteration number}, \textit{iteration vector} and \textit{iteration space} were also introduced. The iteration number is the 
index number for a particular loop iteration, while the iteration vector
extends this concept to a multi level loop nest. Each element in the vector
corresponds to one level in the loop nest with left most element representing the outermost loop index. The set of all possible iteration vectors then
constitute the iteration space. Figure~\ref{fig:inivis} visualizes these concepts with a simple loop nest. 

\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.9\linewidth]{chap6fig/iterationSp.pdf}
\caption{Dependencies in a Loop Nest
\label{fig:inivis}}
\end{center}
\end{figure}

To exploit coarse grained parallelism in the loop nest, we need to ensure the
absence of loop carried dependencies at a particular level in the loop nest. There are also simple transformations(e.g. loop interchange) which can move loop carried
dependencies to other loop levels so only loop-independent dependencies are left. To enable the discovery of these opportunities, we need to find, in each statement's iteration space, if they intersect with other statements' accessed memory locations. In general, the memory addresses to be accessed can be arbitrary functions of loop indices and runtime data, which makes it impossible to statically determine if
dependencies exist. There are, however, a  set of problems where
the memory referenced can be analyzed during compile time as the addresses are affine functions of the loop index variables. For loop nests of this kind,
dependency analysis is essentially finding integer solutions for the problem:

\begin{equation}
\begin{aligned}
\label{dioeq}
& \text{} & & f(\vec{x}) = h(\vec{y}) \\
\end{aligned}
\end{equation}
\begin{equation*}
\begin{aligned}
& \text{ where}  & & f(\vec{x}) = a_0 + a_1x_1+...+a_nx_n \\
& & & h(\vec{y}) = b_0 + b_1y_1+...+b_ny_n \\
& & & \vec{x}_{lb} \le \vec{x} \le \vec{x}_{ub} \\
& & & \vec{y}_{lb} \le \vec{y} \le \vec{y}_{ub}
\end{aligned}
\end{equation*}
Or equivalently this linear Diophantine equation:
\begin{equation}
\begin{aligned}
\label{adioeq}
a_1x_1-b_1y_1+...+a_nx_n-b_ny_n = b_0 - a_0
\end{aligned}
\end{equation}
\begin{equation*}
\begin{aligned}
& \text{ where}  & & x_{lb_k} \le x_k \le x_{ub_k} \\
& & & y_{lb_k} \le y_k \le y_{ub_k} \\
\end{aligned}
\end{equation*}

Both function $f$ and $h$ takes a iteration vector from within the iteration space of the statement, bounded by $\vec{x}_{lb}/\vec{y}_{lb}$ and $\vec{x}_{ub}/\vec{y}_{ub}$, and map it to a memory location. When
multi-dimensional arrays are used, variables can be separated such that
we have multiple simultaneous equations which are simpler to solve.
The loop carried dependency in figure~\ref{fig:inivis}, for instance, correspond to the following equations:
\begin{equation*}
\begin{aligned}
-1+x_1 = -1+y_1 \\
x_2 = y_2-1 \\ 
\end{aligned}
\end{equation*}
\begin{equation*}
\begin{aligned}
& \text{ where}  & & 1 \le x_1 \le 2 \\
& & & 1 \le x_2 \le 2 \\
& & & 1 \le y_1 \le 2 \\
& & & 1 \le y_2 \le 2 \\
& & & x_1,x_2,y_1,y_2 \in Z
\end{aligned}
\end{equation*}


This problem is an integer linear program, one of the NP-complete problems.  
There are various techniques~\cite{banerjee}~\cite{gcd}, proposed over the years, to efficiently solve a relaxed version of this problem. 
%can be used to compute if
%dependencies exist between memory references. Essentially they are all trying %to find solutions for the problem:
%Many techniques were proposed for compiling these problems onto variable %processors. 
In our binary-based flow, we also try to target these analyzable loop nests, some of which are especially amenable to FPGA acceleration. 
Identifying these regions from the program binaries, however, poses some challenges. 
%however, is not as
%simple as doing it from the source code.


%This can be done by solving the

%direction vector
%is the way to do it
%To capture the dependencies between statements, \texit{direction vector} are computed between statements.

\subsection{Challenges for Binary-based Analysis}
\label{sec:cfbba}
The regular and analyzable memory access patterns expressed in high
level languages become rather mangled when the program binaries are 
being examined. All memory accesses are pointer based with no high
level information to indicate if the data structures they are targeting
are disjoint. With dimensionality of the arrays eliminated, 
separate variables in original address calculation are now coupled to each other. In essence, we have to perform dependency analysis on a huge linear
array with all all addresses being the result of some run time computation.
Figure~\ref{fig:mangledMem} illustrates how different a
set of memory accesses manifest themselves in the actual high level source code v.s. program binaries.

\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.8\linewidth]{chap6fig/memBin.pdf}
\caption{Memory Accesses in Program Binaries
\label{fig:mangledMem}}
\end{center}
\end{figure}

In this example, even though the original array references are all affine functions of the loop index variables, in the program binaries, their calculation is much more complex. 
Dependency testing involving only a single loop index in the source code now have to deal with multiple induction variables. More importantly, for equation~\ref{adioeq}, the coefficients ($a_0...a_n$ and $b_0 ... b_n$) are statically known constants while
in the binaries, everything is a runtime data item stored in either a register or a memory location. Naively 
substituting these symbolic variables into the Diophantine's equations yields
a non-linear formulation which can not be solved by the common techniques
in optimizing compilers. On the other hand, %it is relatively straight forward, 
as long as these coefficients are unchanged during the execution of the loop nests, we can leverage various classic dependency testing techniques to formulate a fixed set of checks to be performed during run time. This is possible because the dependency
analysis problem does not scale up with the number of iterations, but rather the number of levels in the loop nest, which is easily recognizable from the static binaries. We therefore can quickly verify our coarse grained parallelization before the invocation of the accelerator, 
avoiding speculative execution and the associated costs. 

In our flow, dataflow analysis is always performed on loop nests to ensure there are no updates to these coefficients during the loops' execution, before more detailed dependency testing and parallelization are attempted. The  potential for actual speed up of course largely depends on the existence of coarse grained parallelism.



%we can be sure that only a fixed amount of computation is needed in
%the dependency. This is not hard to see as the size of the problem formulated 
%does not depend on the actual values of the variable but the number of levels in the loop nest.

%we can assume they are runtime constant and the dependency analysis
%can be carried out. 



%Our approach to deal with this uncertainty is described later in section~\ref{makingAcc}.





\subsection{From Dependency Testing to Parallelization}
\label{sec:fdtp}
%With the dependency testing done, we need to examine the loop nests' potential for parallelization.

To identify the opportunities in parallelizing a loop nest, the dependency distance and direction vectors~\cite{opac-b1000180} are used.
%One way to bridge this gap is through the use of distance and direction vectors~\cite{opac-b1000180}. 

\begin{figure}[htp]
\begin{center}
\includegraphics[width=1.0\linewidth]{chap6fig/paral.pdf}
\caption{Parallelization with Direction Vectors
\label{fig:paral}}
\end{center}
\end{figure}
The dependence distance vector can be computed:
\begin{equation*}
\begin{aligned}
&  & & \vec{d}(\vec{x},\vec{y}) = \vec{y}-\vec{x} \\
& \text{ where }&   & \vec{x} \text{ and }\vec{y} \text{ are solution to equation~\ref{dioeq}}  
\end{aligned}
\end{equation*}

The distance vectors give rise to direction vectors $D(\vec{x},\vec{y})$:
\begin{equation*}
\begin{aligned}
  & & ``<" \text{ if } \vec{d}(\vec{x},\vec{y})_k>0 \\
 \vec{D}(\vec{x},\vec{y})_k = &   & ``=" \text{ if } \vec{d}(\vec{x},\vec{y})_k=0\\
 & & ``>" \text{ if } \vec{d}(\vec{x},\vec{y})_k<0
\end{aligned}
\end{equation*}


The convention is to have the lexicographically earlier iteration vector to be
$\vec{x}$ and with that the leftmost non-``=" element of a direction vector would always be ``$<$". 
The index for this leading ``$<$" is also the level of the  loop-carried dependency. 
Assuming a transformation $T$ does not violate loop-independent dependencies,
~\cite{Kennedy:2001:OCM:502981} proved that $T$ is valid as long as
it does not cause some of the direction vectors to have ``$>$" as the leftmost non-``=" component. In addition, any iteration reordering at a level of the loop not carrying dependency is also valid. Using these proven theorems, we can
decide if loop transformations and iteration parallelization are legal when we know all the direction vectors of a loop nest. Figure~\ref{fig:paral} illustrates a
few scenarios where dependency direction vectors are used to determine validity
of transformation/parallelization.








%To summarize, our flow targets loop nests whose direction vectors can be extracted and coarse grained parallelism identified.



\begin{comment}

we leverage several techniques for taking advantage of the discovered parallelism. For the inner most loop, loop pipelining is always applied
to 

Assuming a particular loop level does not carry dependency, 

Comparing against other parallel computers such as multicore or SIMD machines, the 

Parallelization in FPGA vs others:
vectorization ? separate thread? loop pipelining? 


In order to get the dependency vector however, the memory access patterns would
need to be analyzable. Techniques for extracting dependency vectors were outlined in the .... These techniques generally require the array references
to be affine functions of the loop indices, where the coefficients are
all constants. 
Unfortunately, in the case of accelerating binaries, the coefficients are 
always variables stored in either register or a memory location. Naively
substitutes these variable into the Diophantine's equations yield
non linear conditions...As a result, even if the underlying computation are parallelizable, it is a lot harder to verify with the program binaries
as the starting points. It is relatively straight forward, however,
to detect if these coefficient are changed during the execution of the loop nests. 
\end{comment}

% we want something analyzable ---- % but of course even if the source code
% is analyzable, the binary may not be


\begin{comment}
Schemes for checking 
%

This is certainly compatible with the how conventional FPGAs are used
in most applications, where entire compute intensive loops are offloaded
and sped up.

Even as we focus on speeding up loops with large iteration count,
certain loop nests are just difficult to accelerate using FPGAs.
%especially
%when all higher level information is stripped away, as is the case with program binaries.
Due to the slower frequency of FPGA accelerators, it is not sufficient to just exploit instruction level parallelism. For there to be any net gain
in performance, coarse grained parallelism at the loop level has
to be extracted. To establish independence between different loop iterations or perform transformation to extract coarse grain parallelism, the 
\end{comment}


\section{A Two Phased Approach for Accelerating Program Binaries Using FPGA}
\label{makingAcc}
As the accelerator synthesis, which also includes the traditional FPGA CAD flow,
can take hours to finish, the loop transformation/parallelization need to be performed offline. We therefore have to obtain the direction vectors 
before running the actual program. 
In this work, Banerjee's test~\cite{banerjee} is used to find these vectors. The required coefficients for equation~\ref{adioeq} are extracted from past runtime profiles. The dependency testing and parallelization performed during this \textit{offline phase} are  thus a reflection of the programs' past behavior. When the accelerator is actually running, the input data would have changed. The \textit{online phase} thus includes a mechanism to guarantee the 
semantics of the program is not violated by the reordering performed during
the accelerator synthesis. As we have mentioned in section~\ref{sec:cfbba},
a verifying function is invoked before the accelerator, ensuring the correctness
of the overall execution. 



This online phase test is also one of the reasons why we choose Banerjee's method for our dependency testing even though it is not the most accurate dependency testing method
available. The Banerjee's inequality tests for existence of any real solutions for the Diophantine's equations. Since it only tackles the non-integer relaxation of the ILP formulation, the results obtained are conservative.
It will never report lack of dependencies when one exists,  but may report false dependencies, i.e. when all the real solutions are non-integral.
 More accurate tests like~\cite{omega} find integer solutions but are more costly. Since we are
going to perform the test again using runtime data when the accelerator is actually
being invoked, the faster, though more pessimistic, method is preferred.

When applying Banerjee's method, a particular direction vector $\vec{v}$ is subjected to test, and the result reveals if a pair of memory accesses are dependent in $\vec{v}$'s direction. 
%first used as the constraint and the result of the test reports if this vector may hold true for a pair of memory accesses. 
%Therefore, 
To find all the direction vectors for dependencies, a hierarchy
of tests are therefore involved. For a pair of memory accesses in a two level loop nest, the possible tests are shown in figure~\ref{fig:testingHier}, where ``$\ast$" denotes a union of ``$<$", ``=" and ``$>$". Negative test result from any node in the hierarchy eliminates the necessity to continue testing its subnodes. A subset of these dependency direction vectors
are true for each loop nest and dictates what kind of transformations are valid for accelerator synthesis.


\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.6\linewidth]{chap6fig/testingHier.pdf}
\caption{Hierarchy of Dependency Testing for A Two-Level Loop Nest
\label{fig:testingHier}}
\end{center}
\end{figure}



\subsection{Offline Phase: Accelerator Synthesis}

\subsubsection{Dependency Analysis}
To perform the offline dependency testing, we built an analysis pass within the LLVM framework. As LLVM takes C/C++ as input, a preprocessing step is performed
to convert the binary of the selected loops to C functions. It is also in this step when we extract coefficients for the Diophantine equation and bounds for loop indices from past runtime profiles.
The numbers obtained are substituted as constants into the C functions, enabling the subsequent analysis. 
In the original binaries, these variables are often stored in the memory. 
Thus in addition to examining their past values, we also perform a check to ensure their memory locations do not alias with any other references performed by the loop nest, again using the addresses observed from the profile. Most of the time, these locations can be
recognized as part of the call stack, and are normally completely disjoint from
the memory footprint of the actual loop. We can thus easily disambiguate them using a simple
range test. Of course, when the accelerator is actually being invoked, this test would need to be run again, as will be detailed in section~\ref{onlinephase}.


\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.9\linewidth]{chap6fig/analysisSteps.pdf}
\caption{From Disassembled Binary to Synthesizable C Code
\label{fig:mainSteps}}
\end{center}
\end{figure}

 
Figure~\ref{fig:mainSteps} illustrates the steps involved in converting binary to a synthesizable C function. 
The original output from the disassembler is used to generate two
different versions of C functions. The first, incorporating values collected from past runtime profile, can be analyzed by Banerjee's method. The results are the dependency directions between memory operations. In this particular case, the store and load operations are found to be non-aliasing. This information is then used to generate the final C function, where each memory access is associated with a different pointer. Whether we
feed this version to conventional HLS or our pipeline generation flow, separate pointer
arguments can be mapped to multiple memory ports, through which data requests can be initiated
independently.

%are applied to a loop doing simple vector addition.%example in figure~\ref{fig:mangledMem}.
\subsubsection{Memory Level Parallelism}
\label{subsec:mlp}
There are actually two levels of parallelization when it comes to scheduling memory accesses during HLS. As shown in the example above, the absence of dependencies between memory operations allows for their association with
different pointers. The scheduling engine in the HLS tool can then schedule
them without being constrained by the original program order. However, multiple pointers may still be assigned to a single physical memory  interface. The structural hazard would then dictate the action of the scheduler,
as a single memory port may only accommodate one load and one store per cycle.
To loosen this structural constraint, it is possible to create multiple memory
ports, each assigned to a subset of pointer arguments. More aggressive scheduling, from the datapath's perspective, can then be performed. In addition, certain memory operation inside a loop can be converted to burst
access when it does not have to share the port with others. This frequently boost the efficiency of the off-chip bandwidth usage.


The precondition for exploitation of any memory level parallelism is of course the non-overlapping of addresses accessed by memory operations. This is especially true if they are to be issued through multiple ports, 
%they need to be completely independent from each other 
as we assume the RTL generation backend, the interconnect and memory subsystem may all be aggressively reordering these requests -- by inferring burst accesses or buffering. Therefore, for every pair of accesses whose ordering matters (i.e. excluding load-load pair), we check the direction vector (*,*,...), and associate each access to a separate
pointer if the test result is negative. In section~\ref{subsec:pmd}, we described how memory barriers are inserted to prevent overly conservative dependency annotation.
Similarly, in our binary based flow, if memory dependencies are carried by outer loops,
memory barriers can also be inserted to facilitate the partitioning of memory access interfaces. The direction vector to test here is  (=,=,..,*,*),
where "=" occupies all levels including and outside of the level of insertion. In essence, we are ensuring any reordering of memory operations within the loop level where the memory barrier is inserted will be valid. An example of this is shown in figure~\ref{fig:withOrWithoutBarrier}. 


\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.9\linewidth]{chap6fig/memOrMemBarrier.pdf}
\caption{Partitioning of Memory Access Interface with Insertion of Memory Barriers 
\label{fig:withOrWithoutBarrier}}
\end{center}
\end{figure}


\subsubsection{Coarse Grained Parallelism}


In addition to memory level parallelism, the loop shown in the example also has coarse grained parallelism between loop iterations. In general, a typical high level synthesis flow can employ several common mechanisms to parallelize loops, as illustrated in figure~\ref{fig:fpgaparal}. 
For the inner most
loop, it is generally very cost effective to pipeline the loop--starting a new iteration before the previous one finishes. If
the iterations are parallelizable, the initiation interval would be 1.
Assuming there are $M$ iterations, the total execution time of the loop
would roughly be $M$ cycles. To further reduce this number, loop unrolling
can be performed. It combines multiple iterations into one, effectively
turning inter-iteration parallelism into fine grained parallelism. 
The iteration count is reduced by the unroll factor ($U$) while the II does not change. Consequently, the total execution time is reduced to roughly $M/U$ cycles with a approximately $U$ fold increase in area. 

\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.8\linewidth]{chap6fig/fpgaParallel.pdf}
\caption{Parallelization in FPGA Accelerators
\label{fig:fpgaparal}}
\end{center}
\end{figure}

To improve the throughput of the accelerator even further, multiple
independent datapaths can be instantiated, each performing a subset of 
the iterations. This technique can be used to parallelize outer loops
as well. For loop with a large number of iterations, this technique provides a good way to trade off more on-chip resources for better performance, although
it also incurs some extra synchronization overheads. Another advantage of
having multiple datapaths is the independence between their controllers.
In an earlier chapter(section~\ref{motex}), we illustrated the vulnerability
of a single monolithic schedule to data access latencies and the resulted
underutilization of the memory bandwidth. In the presence of coarse grained parallelism, it is much easier to fully exploit the data bandwidth provided
by the platform, as each datapath independently generates requests, whether
its peers are stalled or not. 
Certain platforms also provide multiple channels
for off-chip memory access, which can be more easily utilized with duplicated datapaths. 


%Figure~\ref{fig:fpgaparal} illustrates the described parallelization schemes for FPGA accelerators.
\begin{figure}[htp]
\begin{center}
\includegraphics[width=1.1\linewidth]{chap6fig/threadSplit.pdf}
\caption{Thread-level Parallelization 
\label{fig:threadP}}
\end{center}
\end{figure}

We can draw similarities between these techniques and the compiler optimizations targeting architectural features
of various parallel processors. Loop pipelining was a scheduling technique
widely used for VLIW machines, where it is often called software pipelining. Loop unrolling, in the synthesis of FPGA
accelerators, is similar to vectorization, as a wider processing engine
is now used to process multiple iterations simultaneously. The replication of
datapath is essentially generating multithreaded implementation from a serial
specification. Previous research in automatic parallelization can thus be
leveraged for our flow. In this work, we are not trying to invent new techniques, but to enable past work to be applied to program binaries on FPGA
platform. In fact, techniques like loop unrolling and loop pipelining are already built
into commercial HLS tools, and can be easily applied using directives. For thread level parallelization, LLVM can again be used to create multiple functions from a single loop nest. As illustrated in figure~\ref{fig:threadP}, the parallelizable dimension of the iteration space
is to be split evenly, a guard condition is also generated to ensure the validity 
of the derived bounds for all the threads.



\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.75\linewidth]{chap6fig/loopInterchange.pdf}
\caption{Loop Interchange Based on Coefficient Value from Runtime Profile 
\label{localityopt}}
\end{center}
\end{figure}
\begin{comment}
It is particularly interesting to look at the interaction between memory level parallelization and coarse grained parallelization. The pipelining of
loops is a form of instruction reordering, in the absence of memory level
parallelism, the amount of speed up achievable is rather limited~\cite{nwcache}. Similarly, when loops are unrolled, more memory operations are exposed in every iteration of the loop. The increased hardware cost is only justifiable if there are enough simultaneous data streams from the memory subsystem to satisfy the multiplied throughput provided by the wider datapath. The same constraint is applicable to thread-level parallelization as well. For regular computation kernels we target, it is
possible to analytically estimate how much coarse grained parallelization
should be performed given a memory bandwidth budget.
\end{comment}


In our flow, the memory footprint of the computation is assumed to be
much larger than the capacity of on-chip RAM. With the fetching of data
from off-chip storage being a major task of the generated accelerator,
thread-level parallelization is preferred to loop unrolling.
Unrolled inner loop often contain multiple memory accesses which were
initially folded as one operation. In a pipelined inner loop, this
memory access is often converted to burst mode load or store, which is
very efficient. Unrolled inner loop, on the other hand, prevents this burst
inference from occurring as the original single stream of data is now broken
into two interleaved streams.
Loop unrolling makes more sense when the on-chip storage is partitioned into small scratchpad memories which can be accessed by the datapath simultaneously.
Given the regularity of the targeted kernel, it is certainly possible to create a mechanism through which off-chip data gets prefetched and distributed into many distributed on-chip RAM. However, for the benchmarks where off-chip communication 
is the main performance bottleneck, there is little benefit in doing this. 
Also, the overhead of multiple datapath is only marginally higher than unrolled
loop as the main overhead is only the extra control state machine.  



\subsubsection{Optimization for Data Locality}
While FPGAs can exploit parallelism in 
multiple different ways, another important aspect to optimize for would be the temporal and spatial locality of data access. While multiple datapaths can
easily saturate the offchip memory bandwidth, there might be wastage if opportunities for data reuse are missed. 
Even in the absence of data reuse, requests for 
contiguous segments or memory locations close to each other are more efficiently served by the caches and the memory subsystem.
Thus in our flow, we also try to increase the ratio of computation per unit of data communicated with off-chip RAM. By performing loop interchange under the dependency constraints, we can generate multiple versions of a loop nest. To find the implementation with best locality, we would like to have the version with the smaller memory footprint (normalized by iteration count) at the inner loop level.
For our targeted loop nests, the memory addresses accessed are affine expressions of the loop index variables. The coefficient for each of them
decides how large of a stride each iteration takes, which conveniently
approximate how ``unlocal" the memory accesses are. 
Thus for a particular loop index variable, the smaller the coefficient, the deeper its corresponding loop level should go.
This is illustrated in figure~\ref{localityopt}. 



The algorithm used to perform code transformation in our flow is outlined in algorithm~\ref{algoPara}. This is a modified version of the \textit{codegen}
procedure from~\cite{Kennedy:2001:OCM:502981}, which targets vector machines.
Our implementation, in addition to data level parallelism, also try to expose
thread level parallelism at the outer level loop, which can then be exploited by multiple FPGA datapaths. 
The results from our algorithm only provides a starting point for the design space exploration. Even for a single vectorized instruction, combinations of techniques illustrated in figure~\ref{fig:fpgaparal} can be applied to
produce implementations of different performance area trade-off. Meanwhile, 
the supporting infrastructure on the FPGA can also be modified on a per
application basis. The parametrization of on-chip buffer, communication mechanism with external
storage etc. are all parts of the design space. The co-tuning of these ``glue"
structures in conjunction
with the accelerator itself is left to future work.  In section~\ref{biev}, we sample a small subset of the possible design points
to show the trade-off between different metrics.


There are also other transformations which can be applied to extract additional parallelism or to achieve better data reuse. Loop fission and loop tiling, for instance, are techniques used in some optimizing compilers. 
Their incorporation is certainly feasible with our general approach and will
add more dimensions to the design space. 
\begin{comment}

The actual implementation of the parallelization engine is built
on top of the LLVM framework. As LLVM takes C/C++ as input, we created a preprocessing step
to convert the binary of the selected loops to C functions.
It is also in this step when we extract
coefficients for the Diophantine equation and bounds for loop indices from past runtime profiles.
The numbers obtained are substituted as constants into the C functions, enabling the subsequent analysis. 
In the original binaries, these variables are often stored in the memory. 
Thus in addition to examining their past values, we also perform a check to ensure their memory
locations do not alias with any other references performed by the loop nest, again using the
addresses observed from the profile. Most of the time, these locations can be
recognized as part of the call stack, and are normally completely disjoint from
the memory footprint of the actual loop. We can thus easily disambiguate them using a simple
range test. Of course, when the accelerator is actually being invoked, this test would need to be
run again, as will be detailed in section~\ref{onlinephase}.


FIXME:There are many existing compiler passes within LLVM which facilitates the analysis ....
After the transformations, the LLVM IR is converted back to C, before HLS is used
to generate the final circuits. Multiple independent C functions can be generated
when multiple datapaths are desired in the final implementation. Vendor specific
pragmas are also inserted to provide guidance for the HLS tool. An example output,
which targets Vivado HLS backend, is shown in figure~\ref{}. 
\end{comment}

\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.85\linewidth]{chap6fig/sourceCodeAnalysis.pdf}
\caption{Analyze Loop in C with Value from Past Profile 
\label{fig:srcAna}}
\end{center}
\end{figure}

It is worth noting that the approach taken in our \textit{offline phase} can actually parallelize certain functions which are not analyzable using only the source code. An example of this is shown in figure~\ref{fig:srcAna}. As the data structures
are passed into the function using pointers, the constant terms (a, b and c) and the coefficient (dim) in the Diophantine equations are all unknown. Banerjee's method, in this case, does not produce affirmative result. On the other hand, with the help of runtime profile,  this function can also be analyzed and produce negative
outcome when tested for the dependency vector (*) -- the loop is parallelizable. 

So far the analysis and parallelization are all performed independent of the actual
RTL generation. In essence we have devised a set of transformations applicable within the LLVM framework. A piece of disassembled binary is converted to synthesizable C code, which is ready to be processed by the hardware synthesis mechanism, be it conventional HLS or the previously described pipeline generation flow. The last part of the \textit{offline phase} involves pushing the RTL through traditional FPGA CAD flow. This step can take between tens of minutes to a few hours,
depending on the size of the design and the utilization of the FPGA chip. 


\begin{comment}
c 2 c
independnet


Even though their location can usually be recognized 
as part of the call stack, we still check for aliasing between 


There are various transformation (e.g. loop interchange passes already
built into LLVM
\end{comment}
%In this work, however, we aim to 

%In addition, as the hardware on FPGAs can be parameterized on a per application basis, there is a huge design space where the high level input
%for HLS as well as the supporting infrastructure (e.g. on chip buffer) can
%be co-tuned. A thorough exploration of this space is left to future work.

%are affine expression of the loop indices, each with a different coefficient,
%The range of memory
%locations accessed by each load/store instructions under each loop level
%is used as a rough approximation of locality. To make the comparison fair, this
%range is also divided by the number of iterations in the corresponding levels.





\subsection{Online Phase: Parallelism Validity Check and Accelerator Execution}
\label{onlinephase}
\begin{comment}
When applying Banerjee's method, a particular direction vector $\vec{v}$ is subjected to test, and the result reveals if a pair of memory accesses are dependent in $\vec{v}$'s direction. 
%first used as the constraint and the result of the test reports if this vector may hold true for a pair of memory accesses. 
%Therefore, 
To find all the direction vectors for dependencies, a hierarchy
of tests are therefore involved. For a pair of memory accesses in a two level loop nest, the possible tests are shown in figure~\ref{fig:testingHier}, where ``$\ast$" denotes a union of ``$<$", ``=" and ``$>$". Negative test result from any node in the hierarchy eliminates the necessity to continue testing its subnodes.  


\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.6\linewidth]{chap6fig/testingHier.pdf}
\caption{Hierarchy of Dependency Testing for A Two-Level Loop Nest
\label{fig:testingHier}}
\end{center}
\end{figure}
\end{comment}
During the \textit{offline phase}, when the direction vectors based off past runtime profiles are assumed to be true, various transformations are performed. If any of these
assumptions is wrong, the generated accelerator will produce wrong results and should not be run. The \textit{online phase} therefore has to verify all the assumptions. More specifically, it needs to check for the presence of any direction vectors which would
invalidate the instruction reordering the \textit{offline phase} has done. 
There a few main components involved in devising this online checking mechanism:
\begin{enumerate}
    \item Constructing the list of vectors to test.  
    \item Generating C code to test each vector. 
    \item Compiling the vector testing C code into a software routine ($SW_{check}$) or a small hardware engine ($HW_{check}$)
    \item Running $SW_{check}$ or $HW_{check}$ created in the previous step
\end{enumerate}

The first three preparatory steps are all done offline,  after the synthesis of the accelerator. The last part though, is invoked right before the execution of the
accelerator. Its result determines if the original software binary or the generated
hardware accelerator should perform the computation.

To enumerate the direction vectors to be checked, our flow examines all the 
transformations performed during the \textit{offline phase}. These 
%performed during the \textit{offline phase} 
include memory-level parallelism, loop interchange and thread-level parallelization. Loop pipelining is
applied conservatively by Vivado HLS and in most cases, aggressive reordering only happens after separation of memory ports. Its validity check is therefore not invoked independently.  

To ensure memory operations can be reordered or even issued through multiple ports, they need to be completely independent from each other as we have explained previously. The vectors tested during the \texit{offline phase}, (*,*...) or (=...,*...) with barrier insertion, are again included for online testing. For thread-level parallelization,
we have to ensure the level at which the iteration space is split does not carry dependency. That is, for any load-store or store-store pair, the direction vector
cannot have leading $``<"$ at this level. An example is shown in figure~\ref{}.

For loop interchange to be valid, the resultant direction vectors would need to have
leading elements not being $``>"$. To achieve this, we first check if the element being moved outward is $``>"$. If it is, then we check if all the levels outside of it are $``="$. If the test result is affirmative, then the loop interchange has violated the 
original program order and the accelerator should not be invoked. This process
is illustrated in figure~\ref{}.

One efficiency optimization, which
is applicable for both phases, is to replace Banerjee's test with a simple address range test. When two data structures occupied different ranges in the address space, it is
computationally very cheap to just compare the upper and lower bounds of the accessed
addresses.

\begin{comment}

we assume the RTL generation backend, the interconnect and memory subsystem may all be
%aggressively reordering these requests -- by inferring burst accesses or buffering. Therefore, for every pair of accesses whose ordering matters (excluding load-load pair), we check the direction vector (*,*,...), and only continue to execute the accelerator when the test result turns out to be negative. There are also cases where this requirement can be relaxed. In section~\ref{subsec:pmd}, we described how memory barriers are inserted to prevent overly conservative dependency annotation.
%Similarly, in our binary based flow, the online check can be less conservative
%when the ordering of memory operations, each associated with different ports, is partially enforced by barriers.
%An example of this is shown in figure~\ref{fig:withOrWithoutBarrier}. 

In separating 



Memory barriers naturally preclude thread-level parallelization at its level in the loop nest, but memory ports can now be separated as long as the dependency is   carried at or outside the level of insertion. To determine where these barriers
are to be inserted, we uses
 




In a lot of regular computation kernels, data from a multi-dimensional array is read, processed and then written into a different array. Using Banerjee's test, we can obtain negative result for the direction vector (*,*,...), but an easier

Table~\ref{tab:transftest} lists
example tests we need to perform for a three level loop when various transformations were applied.

\begin{table}[htbp]
\caption{Direction Vector Checking for Transformations}
\centering
\begin{tabular}{| c | c | c |  }
  \hline            
 \cline{1-3} 
 Example     & Sequence of Direction  & Description of \\
 Transformations     & Vectors to Disprove & Conditions to Satisfy \\
  %
  %                 & LUT &FFs&  BRAM &  LUT &   FFs      & BRAM   \\
  \hline    
  Separate Memory Ports & (*,*,*)& No dependency\\
  w/o Barriers & &  at every level\\
  \hline
  Separate Memory Ports & (*,*,*) $\rightarrow$ ($>$,*,*) & No dependency below \\  
   with Barriers at level 2 & & the level with barrier\\
  \hline
  Parallelization at & (*,*,*) $\rightarrow$ ($<$,*,*) &\\
  the outermost level & &\\
  \hline     
  Loop Interchange & &\\
 
\hline                                                                                                           

\end{tabular}
\label{tab:transftest}

\end{table}
\end{comment}

%These tests certainly introduce runtime overhead into our system. 
Depending on how much actual computation is going on in the target loop nest, these tests may be incurring a significant runtime overhead, especially if they are run on the processor. On the other hand, these tests are highly parallelizable and can be potentially
offloaded to the hardware completely. In either case, these checks reduce the overall
speed up attainable with the accelerator. %Together with the cost of data transfer, which will be described in section~\ref{dtransfer}, these overheads can nullify the benefit of computation offloading. 
Fortunately, we can statically devise a simple model to
evaluate if the amount of computation justifies the invocation of the validity check and accelerator:
\begin{equation}
\label{checkWorth}
& & S_t(I)-H_t(I) >>  D_t
\end{equation}
$S_t(I)$ and $H_t(I)$ are a pair of simple functions estimating the execution time of the loop nest as software binary and hardware accelerator respectively, while $D_t$ denotes the 
runtime overhead of performing parallelization validity check, be it in $SW_{check}$ or $HW_{check}$. $D_t$ does not
vary with the size of the iteration space $I$, while both $S_t$ and $H_t$ are normally simple functions of $I$. 
%given the size of the iteration space. Meanwhile $T_t$ and $D_t$ are functions estimating the time used for data transfer and the parallelism validity check. 
This model is again generated during the offline phase and executed during the online phase.
An example model is shown in figure~\ref{}. Evaluating this model takes time as well, but since it can be reduced to a set of quick checks on the loop bounds, its impact ($M$) on execution time would be rather small.


The main steps for the execution of the accelerator-augmented program binaries is illustrated in figure~\ref{pieces-doModel-doOnline-doInvoke}. As the cost model is always evaluated, in the worst case, when
the accelerated version of the loop nest is never executed, there would be a slight increase of $M$ in execution time. It is therefore advisable to only apply our approach to loop nests whose past behavior exposes sufficiently sized iteration space.



%The performance estimates shown in figure~\ref{fig:fpgaparal} do not take into account the data access latency
%if they have to be brought in from off-chip RAM. By performing transformations
%to maximize data reuse, it can increase the ratio of computation v.s. off-chip memory bandwidth usage. 
\subsection{Data Transfer and Address Translation}
\label{dtransfer}
\subsubsection{Communication between CPU and FPGA Memory Spaces}
Another particular important aspect in ensuring the efficient execution of the accelerators is in the management of data transfers. %For the systems we are targeting, the CPU and accelerator can access the shared address space, explicit movement of data may not be necessary. On the other hand, 
As the FPGA are used as an accelerator for binaries executed on CPU, both the source
and final destination of its working datasets are the address space of
the processor. Depending on the actual configuration of the system, the processor memory may or may not be directly accessible by the FPGA, as discussed in section~\ref{chartarg}. We thus need to provide mechanisms to enable
the sharing of data between the two substrates.


For the cases where the FPGA can directly access the 
CPU's address space, which are the primary targets of our flow, the easiest way is to apply the transformation and
optimizations described in chapter~\ref{decoupleChap}. This would
naturally create a set of address generators who pipeline requests into the 
memory subsystem, as can be seen from figure~\ref{show3demuPipeline}. To benefit from the locality-driven optimization we 
have performed, caches can also be inserted between the address outputs and the
off-chip memory. 

In a more complex scenario, the FPGA has its own address space into which program data need to be transferred first before accelerator execution can occur. The output from the accelerator
is also moved explicitly back to the host CPU's address space.
%and a hardware module accessing the CPU memory directly in master mode is implementable. 
Many of the state-of-the-art platforms with this model
are connected to the host through PCI-e connections and both FPGA vendors allow for
OpenCL to be used to program some of these systems. We can thus express
a mechanism based on OpenCL's data communication model. 
The steps needed include: 
\begin{itemize}
    \item Compute the range of memory addresses to be transferred to and from the FPGA device
    \item Memory allocation on the target FPGA board: $clCreateBuffer$ 
    \item Data transfer to FPGA: $clEnqueueWriteBuffer$
    \item Invoke accelerator: $clEnqueueNDRangeKernel$
    \item Data transfer back to CPU address space: $clEnqueueReadBuffer$
\end{itemize} 

It is worth noting that shared virtual memory (SVM) support has been introduced in OpenCL2.0 where the data movement between the host CPU and the FPGA device can be implicitly handled by the runtime. However, even
in platforms supporting SVM, a buffer is to be allocated explicitly and populated with the appropriate data. For our 
binary based flow, there is still a need to perform the address range computation, memory allocation and kernel(accelerator) invocation. The difference is that now the explicit data transfer between devices gets
replaced by memcpy between the already allocated memory of the original program binary and the SVM buffer just created. 


The analyzability required for coarse grained parallelism discovery 
also enables the \textit{a priori} calculation of the  memory footprint for the accelerated loop nests. As the referenced addresses are always affine functions of the loop indices, for a given part of the iteration space, the boundaries of the accessed range of memory by an instruction can always be
computed. This computation would become another part of the $online$ $phase$, supplying parameters for the actual data movement mechanisms. 


Depending on
the exact computation, the compute of memory ranges and transfer of data
may involve certain trade-offs.
In the best case scenario, every address between the lower and higher boundaries is accessed by the accelerated loop nests. A single memcpy ...
sparse, multiple memcpy may reduce wasted bandwidth for transfer, but increases the overhead in invocation data transfer mechanisms. Application-specific exploration



With the data finally resident in the FPGA's address space, every original access can be translated to a memory reference in the new context by adding/removing an offset. Depending on how the original working
dataset is broken down into memory objects, this offset is computed differently. An example is shown in  figure~\ref{a-memory-reference-get-offseted}. 
In essence, the data transfer mechanism constructs functions for mapping addresses in CPU memory to memory objects accessible by FPGAs. 
%tuples of (memory object, address) in FPGA memory space. 
The affine function mapping points in the iteration space to addresses in CPU memory gets transformed, through adding offsets, to address data
in the new device. 

%composed with these newly constructed function to 


\subsubsection{Handling Virtual Addresses}
When virtual addressing is involved, another layer of complexity is added to the communication between the hardware accelerator and the CPU's memory address space. The CPU's address translation mechanism is normally not accessed by the FPGA, whose outgoing memory requests therefore must use
physical addresses. The generation of addresses by the FPGA accelerator, however, was based off virtual addresses extracted from the software running on the CPU before the accelerator's invocation. It thus becomes necessary to map the virtual addresses generated by the hardware logic to the physical addresses used by the memory subsystem.

In the case where the CPU's memory is not accessible to FPGA
directly and explicit data movement is needed, as just described above, this is less of a problem. One part of the translation happened on the CPU side when the transfer occurs, the virtually referenced data structure get extracted and placed contiguously into the FPGA's physically addressed memory. The remaining part of the translation, as described in part~\ref{dtransfer}, is merely associating references with new memory objects and adding offsets -- a simple and inexpensive process.

On the other hand, if the FPGA is to directly generate the addresses used to access the CPU's memory, virtual to physical address translation would need to happen. As the processor's memory management unit is normally not directly accessible to the FPGA, its functionality needs to be handled using the programmable logic. Several different mechanisms have be devised in
previous projects~\cite{6718414}\cite{7459405}\cite{4042434}, some acting
as a full replacement of the MMU while others take advantage of the assumed
memory access pattern to simplify the hardware. In our case, as we have

\begin{comment}

The range of addresses we would like to copy
to the FPGA device needs to be explicitly specified, the size of the


We need to explicitly specify the range of address we would like to copy to the
FPGA device from the CPU
In this case, we rely on the regularity of the memory access patterns in our targeted loop nests to derive a concise representation of the accessed address stream. 




allows us to easily decouple the management of data transfer from the actual computation.
We can derive concise representations of the accessed addresses by the accelerator without actually running it. 
If the FPGA does not have direct access to the CPU memory, then explicit
data transfer is required. This is usually higher cost






Locality driven optimizations performed
described in section~\ref{}




The analyzability required for coarse grained parallelism discovery 
already provided us with 

also
facilitates the calculation of the exact memory footprint for the accelerated computation. 


More specifically, the addresses stream can be 




In addition, many processor centric systems 
use virtual memory to facilitate programming and manage the memory hierarchy, 






The approach described in chapter~\ref{decoupleChap} may be used but provides little advantage for these computation. 






 

%with 
%a particular parallel architecture as the target may be unnecessary. For %instance, vectorizing compilers may perform loop interchange to have 

%An interesting aspect of 

Our parallelization engine attempt to  

\end{comment}

\begin{comment}
The variety of hardware templates gives another dimension in the accelerator design space, in addition to the software domain optimizations like loop interchange, loop tiling etc. 

For a specific loop nest, whose dependence graph is constructed using the information acquired through dependency analysis, there are multiple
possibilities 

As mentioned in section~\ref{sec:fdtp}, we use Banerjee's tests to find 
direction vectors between different statements in the loop nest. There are 
a few additional techniques employed to make runtime check cheaper





One particular important aspect, in ensuring the efficient execution of the accelerators, is in the management of data transfers. 
The regularity of the memory access patterns in our targeted loop nests
allow us  to easily decouple the transfer of data from the computation. 
The analyzability required for coarse grained parallelism discovery also
facilitates the calculation of the exact memory footprint for the accelerated computation. More specifically, the addresses stream can be 


The approach described in chapter~\ref{decoupleChap} may be used but provides little advantage for these computation. 
\end{comment}

\begin{comment}
At a high level, the approach we are proposing in accelerating program binaries
can be divided into two phases. For an acceleratable loop nest, we first examine
it's past execution profile to extract necessary information to perform dependency analysis offline. After obtaining the dependency direction vectors between statements, we can then perform parallelization and the actual accelerator synthesis. 
Now, whether the generated accelerator is consistent
with the semantics of the original program depends on if the dependency
testing results remain true when the actual execution happens. This is our
second phase, where an online check is performed to confirm the validity
of the assumed parallelism.
\end{comment}








\newpage


\newpage
\newpage
\section{Accelerator Integration through Binary Rewriting}
To replace the loop nest in the original program binary with the generated
validity check and accelerator invocation subroutine, DyninstAPI is again used to perform the modification. Dyninst is capable of performing both
dynamic instrumentation and static rewriting of program binaries. In our flow, we use it as a static binary rewriter so the parsing and instrumentation overhead is amortized.

More specifically, 
Dyninst allows for function replacement at specific call sites. We leverage this to only replace a loop nest, or more precisely, the loop nest's container function, when it is called from a set of predetermined points. 
During the collection of runtime profile, a map was built to associate the amount of computation a particular loop nest performs with the invocation locations of its container function. Only for call sites which triggers large amount of
computation, as certified by equation~\ref{checkWorth}, are redirected to the new implementation.   


of predetermined points

During the profile phase, we built a table, mapping 
We look at the container function of the targette
\subsection{Rewriting flow}



\subsection{Transferring Register Values}

\subsection{}


\section{Experimental Evaluation }
\label{biev}
talk about overhead in running the banerjee's test runtime
software, hardware
compare to range test

model evaluation---tiny bit


design space exploration -- using stupid data path generation







\section{Discussion and Future Work}
If OpenCL is actually being used to perform hardware generation, the analysis work we have done to extract coarse grained parallelism would still work.
At the same time...packaging the transferred data into the memory objects can affect the final performance as well. Having everything in a single 
object in openCL's global memory space ------ not good, less memory level parallelism even though the data transfer between host and FPGA platform
is easier. 




\begin{comment}

%When the input data are changed, the generated accelerator may actually violate
%the semantics of the program. As we have mentioned in section~\ref{sec:cfbba},
%a verifying function is invoked before the accelerator to ensure the correctness
%of the overall execution. Besides, the \textit{online} phase also need to 



%Now the dependency testing results and the subsequent parallelization are a reflection of the programs' past behavior which
%may or may not hold true when the input data are changed. This is when the
%second phase 






%To actually obtain the direction vectors from equation~\ref{adioeq},
%we use Banerjee's approach~\cite{banerjee}.
%To actually solve equation~\ref{adioeq}, we use Banerjee's approach~\cite{}, which find the solution for the non-integer relaxation of the ILP formulation. It does
%which requires constant coefficients. 
%As the coefficients in the equation are not known from the static binary, our flow examines the runtime profiles to extract the values.
%The direction vectors thus obtained can be used evaluate how much coarse
%grained parallelism 



%Their values are extracted from the runtime profile of the programs.
Therefore, the dependency testing results are a reflection of the programs'
past behavior, which may or may not hold true when the input data are changed.
However, given a new set of runtime variables, the amount of computation needed to confirm the results of earlier dependency testing would be constant. This is not hard to see as the size of the problem formulated 
does not depend on the actual values of the coefficients. We now have the computation which has $O(1)$ cost yet guarantees the validity of the extracted
parallelism.


Meanwhile, it is worth noting that Banerjee's inequality only tests for existence of \textit{ real} solutions for the Diophantine's equations. Since it only tackles the non-integer relaxation of the ILP formulation, the results obtained are conservative. False dependencies are reported if all the real solutions are non-integral. More accurate tests like~\cite{omega} find integer solutions but are more costly. As we are
going to perform the test using runtime data when the accelerator is actually
being invoked, the faster, though more pessimistic, approach is preferred.





To actually obtain the direction vectors from equation~\ref{adioeq},
we use Banerjee's approach~\cite{banerjee}.
%To actually solve equation~\ref{adioeq}, we use Banerjee's approach~\cite{}, which find the solution for the non-integer relaxation of the ILP formulation. It does
%which requires constant coefficients. 
As mentioned in section~\ref{sec:cfbba}, the coefficients in the equation, despite being unchanged during the loop execution, are not known from the static binary. Our flow examines the runtime profiles to extract the coefficient values
%Their values are extracted from the runtime profile of the programs.
Therefore, the dependency testing results are a reflection of the programs'
past behavior, which may or may not hold true when the input data are changed.
However, given a new set of runtime variables, the amount of computation needed to confirm the results of earlier dependency testing would be constant. This is not hard to see as the size of the problem formulated 
does not depend on the actual values of the coefficients. We now have the computation which has $O(1)$ cost yet guarantees the validity of the extracted
parallelism.


Meanwhile, it is worth noting that Banerjee's inequality only tests for existence of \textit{ real} solutions for the Diophantine's equations. Since it only tackles the non-integer relaxation of the ILP formulation, the results obtained are conservative. It will never report lack of dependencies when one exists,  but may report false dependencies -- when all the real solutions are non-integral. More accurate tests like~\cite{omega} find integer solutions but are more costly. As we are
going to perform the test using runtime data when the accelerator is actually
being invoked, the faster, though more pessimistic, approach is preferred.







As we mentioned in section~\ref{sec:cfbba}, for loops with analyzable memory accesses, we can perform run time check to validate parallelization we performed in creating the accelerators. This 

As we use past execution profiles to extract parallelism and then perform runtime checks to confirm the validity of our parallelization, our approach
in accelerating program binaries can be divided into two phases. The offline
phase (compile time) consists of dependency analysis, accelerator synthesis and accelerator driver generation. Meanwhile, the parallelism validity check, data transfer and invocation of the accelerator are all performed during the online phase (run time). Being part of the driver, the actual code segments to perform validity check and data transfer are all generated during compile time. As our primary objective is to boost the performance of the accelerated loop nests, the flow tries to create simple and fast run time code whenever it, at the expense
of the compilation time and at times, the accuracy of the run time mechanisms.
\end{comment}