\chapter{Background and Related Work}
\label{relatedWorkChap}
In section~\ref{hlsDevel}, we first briefly discuss the history of high level synthesis 
and its most recent development. As current generation of HLS tools are heavily leveraging
technologies developed in compilers for parallel computers, section~\ref{relevantTech}
describes some of the most relevant work in this area. 
Section~\ref{hetero} looks at previous works in 
integrating conventional processors and reconfigurable fabric, which
provide the target substrate for any software + accelerator
compilation flow. Finally, parallelization based on program
binaries (section~\ref{binpar}) is another research area which overlaps with our effort, though the existing work mostly focused on creation
of parallel executable for multicore/vector machines.


\section{Development of High Level Synthesis}
\label{hlsDevel}
The synthesis of hardware accelerator from higher-level specifications 
has attracted interest from the CAD research community since the 1970s~\cite{Barbacci:1973:AED:906664}~\cite{Parker:1979:CDA:800292.811694}.
The very early works focused on synthesis and simulation at both RTL and algorithmic level, but had little impact outside of academia as 
they predate the emergence of the EDA software industry. During the 80s and 90s, many fundamental concepts in HLS were explored. The main steps in HLS: allocation, scheduling and binding were all being investigated and influential papers and books were published, laying the foundation for the field~\cite{Paulin:1987:FSA:37888.37918}\cite{Park:1986:SPS:318013.318086}\cite{Camposano:1991:HVS:574732}\cite{1586344}\cite{31534}\cite{1270207}\cite{75629}\cite{390403}\cite{Karfa:2006:FVM:1126255.1126731}. Meanwhile, for design input, domain specific languages were often used, especially in the DSP-oriented 
projects~\cite{116097}\cite{20869}\cite{Chu89hyper:an}. In terms of commercialization of the research, in an era where RTL synthesis
was just getting acceptance, the notion that HLS could further improve productivity using an unfamiliar input language did not appeal
to most ASIC designers. As major EDA software vendors ventured into this space~\cite{Knapp:1996:BSD:236240}\cite{tools:monet}, however, HLS tools started 
to draw more attention by the early 2000s. But as the input languages for these tools were behavioral HDLs, 
and the quality of results were not superior to synthesized RTL, they did not achieve much success among the RTL designers. 

The most recent generation of HLS tools finally made an attempt to target algorithm and software designers, by adopting
C/C++ as the medium for design input~~\cite{c2silicon:cadence}\cite{forte:cynthesizer}\cite{1033026}. The rise of FPGAs also offered 
a perfect target for high level synthesis. The time to market advantage of FPGAs is nicely enhanced with a fast algorithm to hardware
mapping flow. The quality of results was also boosted as the newer tools leveraged progress in compiler optimizations and parallel computing.
Consequently, we are seeing wider adoption of HLS in recent years~\cite{5209959}. 
%The optimization required to generate the final implementation assumes a certain level of hardware awareness
%The usage model of modern HLS still assumes hardware-awareness in its users, who need to iteratively optimize the
%design, either in the form of pragma insertion or 
On the other hand, the usage model of modern HLS still assumes a certain level of hardware-awareness in the users, 
be it in the form of following the right coding style~\cite{Stitt:2006:CRM:1233501.1233649} or knowing where and when to insert optimization pragmas. 
Independent studies also suggested the necessity of FPGA expertise in the development of 
efficient end-to-end solutions~\cite{Liang:2012:HSP:2215536.2215537}\cite{7082747}. 
There is certainly room for further improvement as the gap between a truely high level design specification and efficient
hardware implementation still exists. 

\section{Compilation Techniques in High Level Synthesis}
\label{relevantTech}
As mentioned in section~\ref{chap1:het}, the performance advantage of FPGA accelerators comes from the massive
parallelism exploited when computations are spatially mapped. The newest generation of HLS tools has been evolving
concurrently with the wide adoption of parallel computers, many of the techniques proposed for parallelizing compilers
were borrowed and applied in HLS.

Since the targeted platforms can vary for automatic parallelization, parallelisms of different granularities need to be
identified to match of characteristics of the machines (e.g. data-level parallelism for SIMD machines, thread-level
parallelism for multicores etc.). All these can be potentially taken advantage of by FPGAs due to their flexibility.

When mapping to FPGAs, it is possible to exploit instruction level parallelism(ILP) within basic blocks of the program without
applying any transformation. Due to the limit of ILP within basic blocks~\cite{Wall:1991:LIP:106972.106991}, however, the speed up 
is not enough to provide a significant advantage versus a general purpose processor where the clock frequency
can be a lot higher than a typical FPGA. To improve the quality of results in FPGA accelerators, execution of different iterations of loops are often overlapped. This is often  performed using software pipelining techniques,
proposed for (very long instruction word)VLIW processors~\cite{Lam:1988:SPE:53990.54022}\cite{Rau:1994:IMS:192724.192731}\cite{Tirumalai:1990:PLE:110382.110438}\cite{Allan:1995:SP:212094.212131}\cite{Ebcioglu:1987:CTS:255305.255317}.  The HLS tools start new iterations before previous ones are completed, and the minimum interval with which a new iteration can be initiated is dictated by the latency of the longest circular dependence in the control dataflow graph. This initiation interval (II) ultimately bounds the overall throughput achievable by the accelerators. Other loop transformations like loop unrolling and splitinng are sometimes used to complement the loop pipelining performed during HLS. Incurring higher area overhead, they improve the performance by either decreasing the total number of iterations, each with more intra-iteration parallelism, or by simplifying the logic in the critical cycle of dependencies. In general purpose commercial HLS tools, these techniques are often applied through
the use of pragmas by the users. Thus to find out where to apply these
techniques often involves interactive design space exploration using the particular
tool.

In certain application domains where memory access patterns
can be statically analyzed, concise mathematical models can be used to represent data dependencies and mathematical 
transformation~\cite{Ancourt:1991:SPL:109625.109631}\cite{putpolyd2work}\cite{polymorewidely}\cite{girbal2006semi}\cite{pugh1991uniform}\cite{wolf1991loop}. These techniques were employed in program compilation targeting parallel machines of different kind% of various memory hierarchies
\cite{Bacon:1994:CTH:197405.197406}\cite{Hiranandani:1991:COF:125826.125886}\cite{feautrier1988semantical}, and are now used by the HLS community~\cite{Zuo:2013:IPC}\cite{Pouchet:2013}\cite{4785342}.


Another body of work explored transformations from imperative descriptions in
high level programs to alternative models of computation(MoC), before mapping to
hardware accelerators~\cite{mat2pn}\cite{c2stream}. MoCs like Kahn process network
are more suitable for implementation in hardware due to the explicit parallelism laid out
in the model. These methodologies are most effective when applied to regular computation kernels
like those in DSP applications. There are also some unaddressed issues 
such as how to close the gap
between the memory model of the new MoC and that of a
general purpose processor, which is essential in the context of
generating closely coupled CPU+accelerator systems.
Other tools requires the user to directly construct application
in more parallel description of computation to facilitate hardware
generation~\cite{hlfp}\cite{5226333}\cite{MARC}. 

An important aspect of the HLS-generated accelerators lies in their interaction
with the data stored in the memory.
It is a common design practice to have data explicitly moved in and out of chip through
instantiation of DMA engines~\cite{vivado_hls:appnoteMMult}, and coordinate the computation with transfer of data
using software. There are also tools~\cite{coram} which hide the 
complexity of managing memory
hierarchy from the user by providing a standard abstraction
of data access interfaces. The FPGA designers can use the
provided primitives to explicitly manage data movement between
the on-board memory and the SRAM on-chip, allowing
locally-addressed memory accesses by the computation pipeline.
A set of regular kernels are synthesized to this architecture
in~\cite{c2coram}, demonstrating its applicability in the context of high level
synthesis.

\section{Hardware Platforms with Reconfigurable Components}
\label{hetero}
In terms of the actual physical substrates containing reconfigurable elements, 
there were many different architectures proposed over the years.  As the
programmable logic portions of the systems vary in size, the models for their use also
differ. Several platforms use programmable logic as function units~\cite{Carrillo:2001:ERU:360276.360328}~\cite{1266409}~\cite{1240967}~\cite{717456}, tightly integrated
with the processor pipeline. These reconfigurable functional units (RFUs) are given access
to the internal states of the processors and can be used to speed up fine grained 
tasks. The proximity of the programmable logic to the CPU also allows for strong interaction
between the accelerated parts of the computation and the software execution flow. 
To better take advantage of the enhanced architecture, compilers~\cite{10.1109/FPGA.2000.2} were constructed to map operations expressed in high level languages to configurations for the RFUs.

In contrast with those highly integrated architectures, some platforms have loosely
coupled reconfigurable arrays, used in co-processor mode~\cite{624600}~\cite{Lee:2000:DIM:344169.344172}~\cite{chips:viipro}. To effectively
leverage the capabilities of the programmable arrays, computation-intensive coarse grained 
tasks need to be extracted and offloaded. 
Even further along this direction of loose coupling,
many commercial reconfigurable devices are actually used in stand-alone FPGA boards~\cite{boards:alpha}~\cite{boards:terasic}.
Their interaction with the host CPU is even more costly but the capacity of the
arrays themselves are usually higher. Traditionally, HDL programming is the preferred
way to access these devices, however high level synthesis tools can also be used~\cite{839323}~\cite{Singh:2013:HPF:2435264.2435268}.


%larger reconfigurable arrays are
%often used in the co-processor model, where

%In contrast with the systems with reconfigurable functional unit~\cite{Carrillo:2001:ERU:360276.360328}~\cite{1266409}, which 
%boasts low communication overhead between the RFU and the processor pipeline, the targeted platform has much greater capacity in accommodating coarser grained accelerators.



\section{Binary Translation and Optimization}
\label{binpar}
Another research area related to our effort is binary translation, where
the program binary gets modified before it is actually executed on the processor.
This process can happen statically~\cite{chen2008static}~\cite{shen2012llbt}, or dynamically while the application is running.
In a lot of cases, the purpose of translating at the program binary level is to
bridge the gap between existing software and newer hardware. In the late 90s and early 2000s, for instance, several VLIW machines provide their own dynamic binary translation (DBT) layer to ensure the compatibility of their processor with existing executables~\cite{Ebcioglu:1997:DDC:264107.264126}~\cite{Dehnert:2003:TCM:776261.776263}~\cite{Baraz:2003:IEL:956417.956550}. 
There are also DBT systems where the target ISA and source ISA are the same~\cite{Bala:2000:DTD:358438.349303}~\cite{deaver1999wiggins}, 
and the translation is performed to optimize the performance of the program.
In the case of just-in-time compilation~\cite{adl1998fast}, the source binary is for a virtual machine. By translating Java bytecode to the native ISA, the runtime can achieve performance
close to that of a natively compiled binary~\cite{Gherardi:2012:JVC:2428224.2428245}.

With the advent of parallel computers, binary translation also gets used to parallelize sequential programs. In~\cite{yang2011feasibility}, feasibility of dynamic binary parallelization is studied. Traces of instructions are parallelized to run on simulator of multicore machines. 
The system described in~\cite{yardimci2008dynamic}  performs both static analysis and dynamic recompilation of ``super-blocks" to better take advantage of CMP and vector machines.
A more recent project~\cite{7363680} aims to dynamically convert the older Intel SSE instruction set to the newer AVX one.
There are also a few projects which perform the parallelization off-line.
Vizer~\cite{cooper2002vizer} provides a framework which statically vectorizes Intel x86 binaries, targeting a very restrictive set of loops. SecondWrite~\cite{Kotha:2010:APB:1934902.1934997} is binary rewriter which generate multithreaded code for affine loops. It also showed there is but a small difference in
performance improvement when the binaries, instead of the source code, are used
as the starting point for parallelization. 

Our approach described in chapter~\ref{instrumentChap} adopts and modifies some of the ideas in these past research projects, making them applicable to FPGA accelerators.


\begin{comment}
Optimizations targeting the native ISA can often be performed during translation, potentially providing performance improvement in a user transparent way. Systems like Dynamo~\cite{} Pin


Thus, most existing
software dynamic translation (SDT) systems, such as Dynamo [14], DynamoRio [15], Transmeta [26],
and Daisy [27], only perform alias analysis in the form of instruction inspection, which disambiguates
two memory references if they access either different memory regions or their addresses have the
same base register and different offsets.
Polyhedral Parallelization of Binary
Code
\end{comment}
