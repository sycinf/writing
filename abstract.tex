% (This is included by thesis.tex; you do not latex it by itself.)

\begin{abstract}

% The text of the abstract goes here.  If you need to use a \section
% command you will need to use \section*, \subsection*, etc. so that
% you don't get any numbering.  You probably won't be using any of
% these commands in the abstract anyway.
As the scaling down of transistor sizing no longer provides boost to processor clock frequency, there has been
a move towards parallel computers and more recently, heterogeneous
computing platforms.
To target the FPGA component in these systems, high-level synthesis (HLS) tools are developed to facilitate hardware generation from higher level algorithmic descriptions.
Despite being an effective method for rapid hardware generation,
in the context of offloading compute intensive software kernels
to FPGA accelerators, current HLS tools do not always take
full advantage of the hardware platforms. Processor centric software implementations often have to be rewritten if good quality of results is desired.



%As high-level synthesis (HLS) moves towards mainstream adoption among FPGA designers, it has proven to be an effective method for rapid hardware generation. However, in the context of offloading compute intensive software kernels to FPGA accelerators, current HLS tools do not always take full advantage of the hardware platforms. Processor centric software implementations often have to be rewritten if good quality of results is desired.

In this work, we present a framework to refactor and restructure compute intensive software kernels, making them better suited
for FPGA platforms. An algorithm was proposed to
decouple memory operations and computation, generating accelerator pipelines composed of
independent modules connected through FIFO channels.
These decoupled computational pipelines have much better throughput due to their efficient use of the memory bandwidth and improved tolerance towards data access latency.
Our methodology complements existing work in high-level synthesis and facilitates the creation of heterogeneous
systems with high performance accelerators and general purpose
processors. With our approach, for a set of non-regular algorithm
kernels written in C, a performance improvement of 3.3 to 9.1x
is observed over direct C-to-Hardware mapping using a state-of-the-art HLS tool.

To ensure the absence of artificial deadlocks in the pipeline generated by our framework, we also formulated an analysis scheme examining various dependencies between operations distributed across different pipeline modules. The interactions between the modules' schedules, the capacity of the communication channels and the memory access mechanisms are all incorporated
into our model, such that potential artificial deadlocks can be detected and resolved \textit{a priori}. The applicability of our technique is not limited to the computational pipeline generated
by our algorithm, but also other networks of communicating processes if their interaction with the channels follows a set of simple rules.

%Using the newly developed flow, we explore how to minimize user effort in generating and integrating FPGA accelerator with the CPU running software.
To push the limit in usability of FPGA platforms, we also explored the generation and integration of accelerators using only program binaries and execution profiles. Assuming no user input, the approach is only applied to more regular applications,
%It makes the FPGA completely 
%transparent to the user and can be used even in the absence of source code. 
%Of course, the scope of applications this approach can be applied to is relatively narrow. Only regular computation kernels can be converted to FPGA accelerators.
%This approach is only beneficial for more regular applications,
where the memory access patterns are analyzable and coarse grained paralellism can be extracted. A run time mechanism is also devised
to ensure the correctness of the parallelization performed during accelerator synthesis. With the help of binary instrumentation tools, it becomes possible to integrate the FPGA-accelerated parts into the original application in a user transparent way. Neither recompilation of the original program nor the source code is required.
This approach is applied to a few benchmarks for which decoupled computational pipelines are synthesized. With memory level and coarse grained parallelization, significant improvement in performance (3.7 to 9x) over general purpose processor was observed, despite the FPGA running at a fraction of the CPU's clock frequency. The run time checking mechanism was also shown to only incur small overhead, especially for loop nests with large number of iterations. 

%Using the computational pipeline synthesis flow  developed in this work, we applied this approach to a few benchmarks. The coarse grained parallelization provides significant improvement in performance over general purpose processor and the run time checking mechanism is shown to only incur small overhead, especially for loop nests with large number of iterations.
%Performing all the analysis based on program binary does introduce more uncertainty
%than a source code based approach. Thus we have devised a two phase mechanism to have parts of the 
%analysis done during the runtime to ensure correctness. The details are described

\begin{comment}

%much better performance. 
%much better throughput due to their
%efficient use of the memory bandwidth and improved tolerance
%to data access latency. 
The methodology complements existing
work in high-level synthesis, easing the creation of heterogeneous
systems with high performance accelerators and general purpose
processors. With this approach, for a set of non-regular algorithm
kernels written in C, a performance improvement of 3.3 to 9.1x
is observed over direct C-to-Hardware mapping using a state-ofthe-art
HLS tool.

general framework for
transforming a sequential program into a network of processes, which are then
converted to hardware accelerators through high level synthesis.
Also proposed is a complementing technique for performing static deadlock analysis of the generated accelerator network.
The interactions between the accelerators' schedules, the capacity of the communication
channels in the network and the memory access mechanisms are all incorporated
into our model, such that potential artificial deadlocks can be detected and resolved \textit{a priori}.
\end{comment}
\end{abstract}
